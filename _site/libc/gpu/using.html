<!DOCTYPE html>

<html lang="en" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Using libc for GPUs &#8212; The LLVM C Library</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=d75fae25" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css?v=2e637bd6" />
    <script src="../_static/documentation_options.js?v=5929fcd5"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Supported Functions" href="support.html" />
    <link rel="prev" title="Building libs for GPUs" href="building.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="using-libc-for-gpus">
<span id="libc-gpu-usage"></span><h1>Using libc for GPUs<a class="headerlink" href="#using-libc-for-gpus" title="Link to this heading">¶</a></h1>
<nav class="contents local" id="table-of-contents">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#using-the-gpu-c-library" id="id1">Using the GPU C library</a></p>
<ul>
<li><p><a class="reference internal" href="#offloading-usage" id="id2">Offloading usage</a></p>
<ul>
<li><p><a class="reference internal" href="#openmp-offloading-example" id="id3">OpenMP Offloading example</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#direct-compilation" id="id4">Direct compilation</a></p>
<ul>
<li><p><a class="reference internal" href="#building-for-amdgpu-targets" id="id5">Building for AMDGPU targets</a></p></li>
<li><p><a class="reference internal" href="#building-for-nvptx-targets" id="id6">Building for NVPTX targets</a></p></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
<section id="using-the-gpu-c-library">
<h2><a class="toc-backref" href="#id1" role="doc-backlink">Using the GPU C library</a><a class="headerlink" href="#using-the-gpu-c-library" title="Link to this heading">¶</a></h2>
<p>Once you have finished <a class="reference internal" href="building.html#libc-gpu-building"><span class="std std-ref">building</span></a> the GPU C library it
can be used to run libc or libm functions directly on the GPU. Currently, not
all C standard functions are supported on the GPU. Consult the <a class="reference internal" href="support.html#libc-gpu-support"><span class="std std-ref">list of
supported functions</span></a> for a comprehensive list.</p>
<p>The GPU C library supports two main usage modes. The first is as a supplementary
library for offloading languages such as OpenMP, CUDA, or HIP. These aim to
provide standard system utilities similarly to existing vendor libraries. The
second method treats the GPU as a hosted target by compiling C or C++ for it
directly. This is more similar to targeting OpenCL and is primarily used for
exported functions on the GPU and testing.</p>
<section id="offloading-usage">
<h3><a class="toc-backref" href="#id2" role="doc-backlink">Offloading usage</a><a class="headerlink" href="#offloading-usage" title="Link to this heading">¶</a></h3>
<p>Offloading languages like CUDA, HIP, or OpenMP work by compiling a single source
file for both the host target and a list of offloading devices. In order to
support standard compilation flows, the <code class="docutils literal notranslate"><span class="pre">clang</span></code> driver uses fat binaries,
described in the <a class="reference external" href="https://clang.llvm.org/docs/OffloadingDesign.html">clang documentation</a>. This linking mode is used
by the OpenMP toolchain, but is currently opt-in for the CUDA and HIP toolchains
through the <code class="docutils literal notranslate"><span class="pre">--offload-new-driver`</span></code> and <code class="docutils literal notranslate"><span class="pre">-fgpu-rdc</span></code> flags.</p>
<p>In order or link the GPU runtime, we simply pass this library to the embedded
device linker job. This can be done using the <code class="docutils literal notranslate"><span class="pre">-Xoffload-linker</span></code> option, which
forwards an argument to a <code class="docutils literal notranslate"><span class="pre">clang</span></code> job used to create the final GPU executable.
The toolchain should pick up the C libraries automatically in most cases, so
this shouldn’t be necessary.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$&gt;<span class="w"> </span>clang<span class="w"> </span>openmp.c<span class="w"> </span>-fopenmp<span class="w"> </span>--offload-arch<span class="o">=</span>gfx90a<span class="w"> </span>-Xoffload-linker<span class="w"> </span>-lc
$&gt;<span class="w"> </span>clang<span class="w"> </span>cuda.cu<span class="w"> </span>--offload-arch<span class="o">=</span>sm_80<span class="w"> </span>--offload-new-driver<span class="w"> </span>-fgpu-rdc<span class="w"> </span>-Xoffload-linker<span class="w"> </span>-lc
$&gt;<span class="w"> </span>clang<span class="w"> </span>hip.hip<span class="w"> </span>--offload-arch<span class="o">=</span>gfx942<span class="w"> </span>--offload-new-driver<span class="w"> </span>-fgpu-rdc<span class="w"> </span>-Xoffload-linker<span class="w"> </span>-lc
</pre></div>
</div>
<p>This will automatically link in the needed function definitions if they were
required by the user’s application. Normally using the <code class="docutils literal notranslate"><span class="pre">-fgpu-rdc</span></code> option
results in sub-par performance due to ABA linking. However, the offloading
toolchain supports the <code class="docutils literal notranslate"><span class="pre">--foffload-lto</span></code> option to support LTO on the target
device.</p>
<p>Offloading languages require that functions present on the device be declared as
such. This is done with the <code class="docutils literal notranslate"><span class="pre">__device__</span></code> keyword in CUDA and HIP or the
<code class="docutils literal notranslate"><span class="pre">declare</span> <span class="pre">target</span></code> pragma in OpenMP. This requires that the LLVM C library
exposes its implemented functions to the compiler when it is used to build. We
support this by providing wrapper headers in the compiler’s resource directory.
These are located in <code class="docutils literal notranslate"><span class="pre">&lt;clang-resource-dir&gt;/include/llvm-libc-wrappers</span></code> in your
installation.</p>
<p>The support for HIP and CUDA is more experimental, requiring manual intervention
to link and use the facilities. An example of this is shown in the <a class="reference internal" href="rpc.html#libc-gpu-cuda-server"><span class="std std-ref">CUDA
server example</span></a>. The OpenMP Offloading toolchain is
completely integrated with the LLVM C library however. It will automatically
handle including the necessary libraries, define device-side interfaces, and run
the RPC server.</p>
<section id="openmp-offloading-example">
<h4><a class="toc-backref" href="#id3" role="doc-backlink">OpenMP Offloading example</a><a class="headerlink" href="#openmp-offloading-example" title="Link to this heading">¶</a></h4>
<p>This section provides a simple example of compiling an OpenMP program with the
GPU C library.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;stdio.h&gt;</span>

<span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="kt">FILE</span><span class="w"> </span><span class="o">*</span><span class="n">file</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">stderr</span><span class="p">;</span>
<span class="cp">#pragma omp target teams num_teams(2) thread_limit(2)</span>
<span class="cp">#pragma omp parallel num_threads(2)</span>
<span class="w">  </span><span class="p">{</span><span class="w"> </span><span class="n">fputs</span><span class="p">(</span><span class="s">&quot;Hello from OpenMP!</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">file</span><span class="p">);</span><span class="w"> </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>This can simply be compiled like any other OpenMP application to print from two
threads and two blocks.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$&gt;<span class="w"> </span>clang<span class="w"> </span>openmp.c<span class="w"> </span>-fopenmp<span class="w"> </span>--offload-arch<span class="o">=</span>gfx90a
$&gt;<span class="w"> </span>./a.out
Hello<span class="w"> </span>from<span class="w"> </span>OpenMP!
Hello<span class="w"> </span>from<span class="w"> </span>OpenMP!
Hello<span class="w"> </span>from<span class="w"> </span>OpenMP!
Hello<span class="w"> </span>from<span class="w"> </span>OpenMP!
</pre></div>
</div>
<p>Including the wrapper headers, linking the C library, and running the <a class="reference internal" href="rpc.html#libc-gpu-rpc"><span class="std std-ref">RPC
server</span></a> are all handled automatically by the compiler and runtime.</p>
</section>
</section>
<section id="direct-compilation">
<h3><a class="toc-backref" href="#id4" role="doc-backlink">Direct compilation</a><a class="headerlink" href="#direct-compilation" title="Link to this heading">¶</a></h3>
<p>Instead of using standard offloading languages, we can also target the CPU
directly using C and C++ to create a GPU executable similarly to OpenCL. This is
done by targeting the GPU architecture using <a class="reference external" href="https://clang.llvm.org/docs/CrossCompilation.html">clang’s cross compilation
support</a>. This is the
method that the GPU C library uses both to build the library and to run tests.</p>
<p>This allows us to easily define GPU specific libraries and programs that fit
well into existing tools. In order to target the GPU effectively we rely heavily
on the compiler’s intrinsic and built-in functions. For example, the following
function gets the thread identifier in the ‘x’ dimension on both GPUs supported
GPUs.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="kt">uint32_t</span><span class="w"> </span><span class="nf">get_thread_id_x</span><span class="p">()</span><span class="w"> </span><span class="p">{</span>
<span class="cp">#if defined(__AMDGPU__)</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">__builtin_amdgcn_workitem_id_x</span><span class="p">();</span>
<span class="cp">#elif defined(__NVPTX__)</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">__nvvm_read_ptx_sreg_tid_x</span><span class="p">();</span>
<span class="cp">#else</span>
<span class="cp">#error &quot;Unsupported platform&quot;</span>
<span class="cp">#endif</span>
<span class="p">}</span>
</pre></div>
</div>
<p>We can then compile this for both NVPTX and AMDGPU into LLVM-IR using the
following commands. This will yield valid LLVM-IR for the given target just like
if we were using CUDA, OpenCL, or OpenMP.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$&gt;<span class="w"> </span>clang<span class="w"> </span>id.c<span class="w"> </span>--target<span class="o">=</span>amdgcn-amd-amdhsa<span class="w"> </span>-mcpu<span class="o">=</span>native<span class="w"> </span>-nogpulib<span class="w"> </span>-flto<span class="w"> </span>-c
$&gt;<span class="w"> </span>clang<span class="w"> </span>id.c<span class="w"> </span>--target<span class="o">=</span>nvptx64-nvidia-cuda<span class="w"> </span>-march<span class="o">=</span>native<span class="w"> </span>-nogpulib<span class="w"> </span>-flto<span class="w"> </span>-c
</pre></div>
</div>
<p>We can also use this support to treat the GPU as a hosted environment by
providing a C library and startup object just like a standard C library running
on the host machine. Then, in order to execute these programs, we provide a
loader utility to launch the executable on the GPU similar to a cross-compiling
emulator. This is how we run <a class="reference internal" href="testing.html#libc-gpu-testing"><span class="std std-ref">unit tests</span></a> targeting the
GPU. This is clearly not the most efficient way to use a GPU, but it provides a
simple method to test execution on a GPU for debugging or development.</p>
<section id="building-for-amdgpu-targets">
<h4><a class="toc-backref" href="#id5" role="doc-backlink">Building for AMDGPU targets</a><a class="headerlink" href="#building-for-amdgpu-targets" title="Link to this heading">¶</a></h4>
<p>The AMDGPU target supports several features natively by virtue of using <code class="docutils literal notranslate"><span class="pre">lld</span></code>
as its linker. The installation will include the <code class="docutils literal notranslate"><span class="pre">include/amdgcn-amd-amdhsa</span></code>
and <code class="docutils literal notranslate"><span class="pre">lib/amdgcn-amd-amdha</span></code> directories that contain the necessary code to use
the library. We can directly link against <code class="docutils literal notranslate"><span class="pre">libc.a</span></code> and use LTO to generate the
final executable.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;stdio.h&gt;</span>

<span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">()</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;Hello from AMDGPU!</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span><span class="w"> </span><span class="p">}</span>
</pre></div>
</div>
<p>This program can then be compiled using the <code class="docutils literal notranslate"><span class="pre">clang</span></code> compiler. Note that
<code class="docutils literal notranslate"><span class="pre">-flto</span></code> and <code class="docutils literal notranslate"><span class="pre">-mcpu=</span></code> should be defined. This is because the GPU
sub-architectures do not have strict backwards compatibility. Use <code class="docutils literal notranslate"><span class="pre">-mcpu=help</span></code>
for accepted arguments or <code class="docutils literal notranslate"><span class="pre">-mcpu=native</span></code> to target the system’s installed GPUs
if present. Additionally, the AMDGPU target always uses <code class="docutils literal notranslate"><span class="pre">-flto</span></code> because we
currently do not fully support ELF linking in <code class="docutils literal notranslate"><span class="pre">lld</span></code>. Once built, we use the
<code class="docutils literal notranslate"><span class="pre">amdhsa-loader</span></code> utility to launch execution on the GPU. This will be built if
the <code class="docutils literal notranslate"><span class="pre">hsa_runtime64</span></code> library was found during build time.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$&gt;<span class="w"> </span>clang<span class="w"> </span>hello.c<span class="w"> </span>--target<span class="o">=</span>amdgcn-amd-amdhsa<span class="w"> </span>-mcpu<span class="o">=</span>native<span class="w"> </span>-flto<span class="w"> </span>-lc<span class="w"> </span>&lt;install&gt;/lib/amdgcn-amd-amdhsa/crt1.o
$&gt;<span class="w"> </span>amdhsa-loader<span class="w"> </span>--threads<span class="w"> </span><span class="m">2</span><span class="w"> </span>--blocks<span class="w"> </span><span class="m">2</span><span class="w"> </span>a.out
Hello<span class="w"> </span>from<span class="w"> </span>AMDGPU!
Hello<span class="w"> </span>from<span class="w"> </span>AMDGPU!
Hello<span class="w"> </span>from<span class="w"> </span>AMDGPU!
Hello<span class="w"> </span>from<span class="w"> </span>AMDGPU!
</pre></div>
</div>
<p>This will include the <code class="docutils literal notranslate"><span class="pre">stdio.h</span></code> header, which is found in the
<code class="docutils literal notranslate"><span class="pre">include/amdgcn-amd-amdhsa</span></code> directory. We define out <code class="docutils literal notranslate"><span class="pre">main</span></code> function like a
standard application. The startup utility in <code class="docutils literal notranslate"><span class="pre">lib/amdgcn-amd-amdhsa/crt1.o</span></code>
will handle the necessary steps to execute the <code class="docutils literal notranslate"><span class="pre">main</span></code> function along with
global initializers and command line arguments. Finally, we link in the
<code class="docutils literal notranslate"><span class="pre">libc.a</span></code> library stored in <code class="docutils literal notranslate"><span class="pre">lib/amdgcn-amd-amdhsa</span></code> to define the standard C
functions.</p>
<p>The search paths for the include directories and libraries are automatically
handled by the compiler. We use this support internally to run unit tests on the
GPU directly. See <a class="reference internal" href="testing.html#libc-gpu-testing"><span class="std std-ref">Testing the GPU C library</span></a> for more information. The installation
also provides <code class="docutils literal notranslate"><span class="pre">libc.bc</span></code> which is a single LLVM-IR bitcode blob that can be
used instead of the static library.</p>
</section>
<section id="building-for-nvptx-targets">
<h4><a class="toc-backref" href="#id6" role="doc-backlink">Building for NVPTX targets</a><a class="headerlink" href="#building-for-nvptx-targets" title="Link to this heading">¶</a></h4>
<p>The infrastructure is the same as the AMDGPU example. However, the NVPTX binary
utilities are very limited and must be targeted directly. A utility called
<code class="docutils literal notranslate"><span class="pre">clang-nvlink-wrapper</span></code> instead wraps around the standard link job to give the
illusion that <code class="docutils literal notranslate"><span class="pre">nvlink</span></code> is a functional linker.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;stdio.h&gt;</span>

<span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">argc</span><span class="p">,</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="o">**</span><span class="n">argv</span><span class="p">,</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="o">**</span><span class="n">envp</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">printf</span><span class="p">(</span><span class="s">&quot;Hello from NVPTX!</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Additionally, the NVPTX ABI requires that every function signature matches. This
requires us to pass the full prototype from <code class="docutils literal notranslate"><span class="pre">main</span></code>. The installation will
contain the <code class="docutils literal notranslate"><span class="pre">nvptx-loader</span></code> utility if the CUDA driver was found during
compilation. Using link time optimization will help hide this.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$&gt;<span class="w"> </span>clang<span class="w"> </span>hello.c<span class="w"> </span>--target<span class="o">=</span>nvptx64-nvidia-cuda<span class="w"> </span>-march<span class="o">=</span>native<span class="w"> </span>-flto<span class="w"> </span>-lc<span class="w"> </span>&lt;install&gt;/lib/nvptx64-nvidia-cuda/crt1.o
$&gt;<span class="w"> </span>nvptx-loader<span class="w"> </span>--threads<span class="w"> </span><span class="m">2</span><span class="w"> </span>--blocks<span class="w"> </span><span class="m">2</span><span class="w"> </span>a.out
Hello<span class="w"> </span>from<span class="w"> </span>NVPTX!
Hello<span class="w"> </span>from<span class="w"> </span>NVPTX!
Hello<span class="w"> </span>from<span class="w"> </span>NVPTX!
Hello<span class="w"> </span>from<span class="w"> </span>NVPTX!
</pre></div>
</div>
</section>
</section>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">libc</a></h1>








<h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Status &amp; Support</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../headers/index.html">Implementation Status</a></li>
<li class="toctree-l1"><a class="reference internal" href="../arch_support.html">Architecture Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../platform_support.html">Platform Support</a></li>
<li class="toctree-l1"><a class="reference internal" href="../compiler_support.html">Compiler Support</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Simple Usage</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../getting_started.html">Getting Started</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced Usage</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../full_host_build.html">Full Host Build</a></li>
<li class="toctree-l1"><a class="reference internal" href="../full_cross_build.html">Full Cross Build</a></li>
<li class="toctree-l1"><a class="reference internal" href="../overlay_mode.html">Overlay Mode</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">libc for GPUs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../uefi/index.html">libc for UEFI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../configure.html">Configure Options</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Development</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../Maintainers.html">LLVM-libc Maintainers</a></li>
<li class="toctree-l1"><a class="reference internal" href="../build_and_test.html">Building and Testing the libc</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dev/index.html">Developer Guides</a></li>
<li class="toctree-l1"><a class="reference internal" href="../porting.html">Bringup on a New OS or Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contributing.html">Contributing to the libc Project</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Useful Links</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../talks.html">Talks</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/llvm/llvm-project/tree/main/libc">Source Code</a></li>
<li class="toctree-l1"><a class="reference external" href="https://github.com/llvm/llvm-project/labels/libc">Bug Reports</a></li>
<li class="toctree-l1"><a class="reference external" href="https://discourse.llvm.org/c/runtimes/libc">Discourse</a></li>
<li class="toctree-l1"><a class="reference external" href="https://discord.gg/xS7Z362">Join the Discord</a></li>
<li class="toctree-l1"><a class="reference external" href="https://discord.com/channels/636084430946959380/636732994891284500">Discord Channel</a></li>
<li class="toctree-l1"><a class="reference external" href="https://lab.llvm.org/buildbot/#/builders?tags=libc">Buildbot</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
  <li><a href="index.html">libc for GPUs</a><ul>
      <li>Previous: <a href="building.html" title="previous chapter">Building libs for GPUs</a></li>
      <li>Next: <a href="support.html" title="next chapter">Supported Functions</a></li>
  </ul></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2011-2025, LLVM Project.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 7.2.6</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 0.7.16</a>
      
      |
      <a href="../_sources/gpu/using.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>